{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. CNN with CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.utils\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # Data Augmentation\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize(32),\n",
    "    \n",
    "    # Data Nomalization\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Normalize a tensor image with mean and standard deviation.\n",
    "# Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels,\n",
    "# this transform will normalize each channel of the input torch.\n",
    "# *Tensor i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "\n",
    "train_data = dsets.CIFAR10(root='./data', \n",
    "                           train=True,\n",
    "                           download=True, \n",
    "                           transform=transform)\n",
    "\n",
    "test_data  = dsets.CIFAR10(root='./data', \n",
    "                           train=False,\n",
    "                           download=True, \n",
    "                           transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_data, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2 Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5),\n",
    "            \n",
    "            # Batch Nomalization\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            \n",
    "            # Batch Nomalization\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64 * 5 * 5, 100),\n",
    "            # Dropout\n",
    "            # nn.Dropout(0.5)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)              \n",
    "        )\n",
    "        \n",
    "        # Weight Initialization\n",
    "        for m in self.modules() :\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # init.xavier_normal(m.weight.data)\n",
    "                init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0)      \n",
    "            if isinstance(m, nn.Linear):\n",
    "                # init.xavier_normal(m.weight.data)\n",
    "                init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0)                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer(x)\n",
    "        out = out.view(-1, 64*5*5)\n",
    "        out = self.fc_layer(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Momentum & Weight Regularization(L2)\n",
    "# optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], lter [200/390], Loss: 1.3217\n",
      "Epoch [2/10], lter [200/390], Loss: 1.0757\n",
      "Epoch [3/10], lter [200/390], Loss: 1.0477\n",
      "Epoch [4/10], lter [200/390], Loss: 0.7904\n",
      "Epoch [5/10], lter [200/390], Loss: 0.7576\n",
      "Epoch [6/10], lter [200/390], Loss: 0.7350\n",
      "Epoch [7/10], lter [200/390], Loss: 0.8679\n",
      "Epoch [8/10], lter [200/390], Loss: 0.6426\n",
      "Epoch [9/10], lter [200/390], Loss: 0.8302\n",
      "Epoch [10/10], lter [200/390], Loss: 0.6421\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, (batch_images, batch_labels) in enumerate(train_loader):\n",
    "        \n",
    "        X = batch_images.cuda()\n",
    "        Y = batch_labels.cuda()\n",
    "\n",
    "        pre = model(X)\n",
    "        cost = loss(pre, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4 Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test images: 72.580000 %\n"
     ]
    }
   ],
   "source": [
    "# 이제부터는 꼭 해주어야함.\n",
    "# Batch Norm과 Dropout은 Train과 Test일 때 서로 다르게 행동함.\n",
    "model.eval()\n",
    "\n",
    "# 반대로 다시 Train을 해야하면\n",
    "# model.train()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    images = images.cuda()\n",
    "    outputs = model(images)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.cuda()).sum()\n",
    "    \n",
    "print('Accuracy of test images: %f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고로 지난 번 수업(4주차, 14번 코드)에서는 정확도 **47.960000%**를 기록"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
