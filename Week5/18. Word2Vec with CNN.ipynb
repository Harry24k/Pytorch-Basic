{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import random\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/score_발열.xlsx\"\n",
    "sheet_name = \"Sheet1\"\n",
    "data = pd.read_excel(filename, sheet_name = sheet_name, header = 0)\n",
    "\n",
    "csv_data = [item.replace(\"#\", \"\").strip() for item in data['Review']]\n",
    "csv_label = data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['발열히 심한거 같은데 여름이라 그런가?..',\n",
       " '발열이좀 심한거 같아서 걱정이에요',\n",
       " '발열이심하더라구요',\n",
       " '발열이너무심한게 제일큰 단점인것 같고 그외에 불편한점은',\n",
       " '발열이...정말...심합니다']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Okt()\n",
    "size = 500\n",
    "\n",
    "doc = []\n",
    "\n",
    "for sentence in csv_data :\n",
    "    results= []\n",
    "    tokens = tokenizer.pos(sentence, norm=True, stem=True)\n",
    "        \n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "    doc.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['발열', '히', '심하다', '같다', '여름', '그', '런가'],\n",
       " ['발열', '이', '좀', '심하다', '같다', '걱정'],\n",
       " ['발열', '심하다'],\n",
       " ['발열', '이', '너', '무심하다', '제일', '크다', '단점', '것', '같다', '그', '외', '불편하다', '점'],\n",
       " ['발열', '정말', '심하다']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(doc, size=size, window=2, min_count=3, sg=0)\n",
    "\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "\n",
    "del model\n",
    "\n",
    "# sentences (iterable of iterables, optional) – The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See BrownCorpus, Text8Corpus or LineSentence in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
    "# size (int, optional) – Dimensionality of the word vectors.\n",
    "# window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "# min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "# workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "# sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "# hs ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "# negative (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "\n",
    "# Loading pretrained Word2Vec\n",
    "# Google's trained Word2Vec : https://code.google.com/archive/p/word2vec/\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "# import gensim\n",
    "# import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "# model = word2vec.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# weights = torch.FloatTensor(model.vectors)\n",
    "# embedding = nn.Embedding.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document to 3-dim Matrix with Word2Vector & Get Max Length of Sentence\n",
    "\n",
    "doc2vec = []\n",
    "max_length = 0\n",
    "\n",
    "for sentence in doc :\n",
    "    temp = []\n",
    "    length = 0\n",
    "    \n",
    "    for word in sentence :\n",
    "        if word in w2v.keys() :\n",
    "            temp.append(w2v[word])\n",
    "            length += 1\n",
    "            \n",
    "    doc2vec.append(temp)\n",
    "    \n",
    "    if max_length <= length :\n",
    "        max_length = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill zeros for fitting size\n",
    "\n",
    "for sentence in doc2vec :\n",
    "    \n",
    "    length = len(sentence)\n",
    "    \n",
    "    while length < max_length :\n",
    "        sentence.append(np.zeros(size))\n",
    "        length += 1\n",
    "\n",
    "doc2vec = np.array(doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211, 12, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = doc2vec\n",
    "label = csv_label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(train_data).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(train_label).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([908]), torch.Size([908, 12, 500]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size() , x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(-1, 1, 12, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(x, y)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader  = DataLoader(dataset=train_data,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True,\n",
    "                           num_workers=1,\n",
    "                           drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 12, 500])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, label = iter(train_loader).next()\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4 Define Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1,16,3), #1*12*500 -> 16*10*498\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,3), #16*10*498 -> 32*8*496\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), #32*8*496 -> 32*4*248\n",
    "            nn.Conv2d(32,64,3),#32*4*248 -> 64*2*246\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2) #64*2*246 -> 64*1*123\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64*1*123,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,3)\n",
    "        )       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(-1,64*1*123)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.5 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], lter [20/90], Loss: 0.7399\n",
      "Epoch [1/50], lter [40/90], Loss: 0.9133\n",
      "Epoch [1/50], lter [60/90], Loss: 0.9621\n",
      "Epoch [1/50], lter [80/90], Loss: 1.1947\n",
      "Epoch [2/50], lter [20/90], Loss: 1.1412\n",
      "Epoch [2/50], lter [40/90], Loss: 1.1714\n",
      "Epoch [2/50], lter [60/90], Loss: 0.9011\n",
      "Epoch [2/50], lter [80/90], Loss: 0.4859\n",
      "Epoch [3/50], lter [20/90], Loss: 1.0252\n",
      "Epoch [3/50], lter [40/90], Loss: 0.8421\n",
      "Epoch [3/50], lter [60/90], Loss: 0.8277\n",
      "Epoch [3/50], lter [80/90], Loss: 0.9643\n",
      "Epoch [4/50], lter [20/90], Loss: 0.9501\n",
      "Epoch [4/50], lter [40/90], Loss: 1.1209\n",
      "Epoch [4/50], lter [60/90], Loss: 0.9303\n",
      "Epoch [4/50], lter [80/90], Loss: 0.9678\n",
      "Epoch [5/50], lter [20/90], Loss: 0.7490\n",
      "Epoch [5/50], lter [40/90], Loss: 0.9057\n",
      "Epoch [5/50], lter [60/90], Loss: 0.6642\n",
      "Epoch [5/50], lter [80/90], Loss: 1.0030\n",
      "Epoch [6/50], lter [20/90], Loss: 1.2429\n",
      "Epoch [6/50], lter [40/90], Loss: 1.1634\n",
      "Epoch [6/50], lter [60/90], Loss: 0.8501\n",
      "Epoch [6/50], lter [80/90], Loss: 0.8352\n",
      "Epoch [7/50], lter [20/90], Loss: 0.7903\n",
      "Epoch [7/50], lter [40/90], Loss: 1.1849\n",
      "Epoch [7/50], lter [60/90], Loss: 0.9337\n",
      "Epoch [7/50], lter [80/90], Loss: 0.9017\n",
      "Epoch [8/50], lter [20/90], Loss: 0.9207\n",
      "Epoch [8/50], lter [40/90], Loss: 0.7433\n",
      "Epoch [8/50], lter [60/90], Loss: 0.9029\n",
      "Epoch [8/50], lter [80/90], Loss: 0.6510\n",
      "Epoch [9/50], lter [20/90], Loss: 1.3205\n",
      "Epoch [9/50], lter [40/90], Loss: 0.8958\n",
      "Epoch [9/50], lter [60/90], Loss: 0.9753\n",
      "Epoch [9/50], lter [80/90], Loss: 0.9711\n",
      "Epoch [10/50], lter [20/90], Loss: 0.9012\n",
      "Epoch [10/50], lter [40/90], Loss: 0.9140\n",
      "Epoch [10/50], lter [60/90], Loss: 1.0485\n",
      "Epoch [10/50], lter [80/90], Loss: 0.9633\n",
      "Epoch [11/50], lter [20/90], Loss: 1.1953\n",
      "Epoch [11/50], lter [40/90], Loss: 1.0206\n",
      "Epoch [11/50], lter [60/90], Loss: 1.1050\n",
      "Epoch [11/50], lter [80/90], Loss: 0.8313\n",
      "Epoch [12/50], lter [20/90], Loss: 0.9016\n",
      "Epoch [12/50], lter [40/90], Loss: 1.5094\n",
      "Epoch [12/50], lter [60/90], Loss: 0.7610\n",
      "Epoch [12/50], lter [80/90], Loss: 0.9965\n",
      "Epoch [13/50], lter [20/90], Loss: 0.9036\n",
      "Epoch [13/50], lter [40/90], Loss: 0.8548\n",
      "Epoch [13/50], lter [60/90], Loss: 1.1268\n",
      "Epoch [13/50], lter [80/90], Loss: 0.8581\n",
      "Epoch [14/50], lter [20/90], Loss: 0.9454\n",
      "Epoch [14/50], lter [40/90], Loss: 1.1000\n",
      "Epoch [14/50], lter [60/90], Loss: 1.1047\n",
      "Epoch [14/50], lter [80/90], Loss: 0.7461\n",
      "Epoch [15/50], lter [20/90], Loss: 1.3986\n",
      "Epoch [15/50], lter [40/90], Loss: 0.8028\n",
      "Epoch [15/50], lter [60/90], Loss: 0.8557\n",
      "Epoch [15/50], lter [80/90], Loss: 0.6963\n",
      "Epoch [16/50], lter [20/90], Loss: 1.1242\n",
      "Epoch [16/50], lter [40/90], Loss: 1.0404\n",
      "Epoch [16/50], lter [60/90], Loss: 0.9896\n",
      "Epoch [16/50], lter [80/90], Loss: 1.0619\n",
      "Epoch [17/50], lter [20/90], Loss: 0.8262\n",
      "Epoch [17/50], lter [40/90], Loss: 1.1676\n",
      "Epoch [17/50], lter [60/90], Loss: 0.8366\n",
      "Epoch [17/50], lter [80/90], Loss: 1.1316\n",
      "Epoch [18/50], lter [20/90], Loss: 1.0730\n",
      "Epoch [18/50], lter [40/90], Loss: 0.9004\n",
      "Epoch [18/50], lter [60/90], Loss: 0.8069\n",
      "Epoch [18/50], lter [80/90], Loss: 0.7755\n",
      "Epoch [19/50], lter [20/90], Loss: 0.7695\n",
      "Epoch [19/50], lter [40/90], Loss: 0.9852\n",
      "Epoch [19/50], lter [60/90], Loss: 1.0230\n",
      "Epoch [19/50], lter [80/90], Loss: 0.8550\n",
      "Epoch [20/50], lter [20/90], Loss: 0.9701\n",
      "Epoch [20/50], lter [40/90], Loss: 1.1385\n",
      "Epoch [20/50], lter [60/90], Loss: 0.8316\n",
      "Epoch [20/50], lter [80/90], Loss: 1.1168\n",
      "Epoch [21/50], lter [20/90], Loss: 0.9344\n",
      "Epoch [21/50], lter [40/90], Loss: 0.7802\n",
      "Epoch [21/50], lter [60/90], Loss: 0.8032\n",
      "Epoch [21/50], lter [80/90], Loss: 0.8288\n",
      "Epoch [22/50], lter [20/90], Loss: 0.8270\n",
      "Epoch [22/50], lter [40/90], Loss: 0.6063\n",
      "Epoch [22/50], lter [60/90], Loss: 1.2090\n",
      "Epoch [22/50], lter [80/90], Loss: 0.6368\n",
      "Epoch [23/50], lter [20/90], Loss: 0.9652\n",
      "Epoch [23/50], lter [40/90], Loss: 1.0343\n",
      "Epoch [23/50], lter [60/90], Loss: 1.0022\n",
      "Epoch [23/50], lter [80/90], Loss: 0.9695\n",
      "Epoch [24/50], lter [20/90], Loss: 1.1274\n",
      "Epoch [24/50], lter [40/90], Loss: 0.8005\n",
      "Epoch [24/50], lter [60/90], Loss: 0.7257\n",
      "Epoch [24/50], lter [80/90], Loss: 0.8109\n",
      "Epoch [25/50], lter [20/90], Loss: 1.1608\n",
      "Epoch [25/50], lter [40/90], Loss: 1.0678\n",
      "Epoch [25/50], lter [60/90], Loss: 0.9977\n",
      "Epoch [25/50], lter [80/90], Loss: 0.9716\n",
      "Epoch [26/50], lter [20/90], Loss: 0.9153\n",
      "Epoch [26/50], lter [40/90], Loss: 0.7357\n",
      "Epoch [26/50], lter [60/90], Loss: 0.7554\n",
      "Epoch [26/50], lter [80/90], Loss: 1.1401\n",
      "Epoch [27/50], lter [20/90], Loss: 0.7515\n",
      "Epoch [27/50], lter [40/90], Loss: 0.5838\n",
      "Epoch [27/50], lter [60/90], Loss: 1.1454\n",
      "Epoch [27/50], lter [80/90], Loss: 1.0159\n",
      "Epoch [28/50], lter [20/90], Loss: 1.1352\n",
      "Epoch [28/50], lter [40/90], Loss: 0.6183\n",
      "Epoch [28/50], lter [60/90], Loss: 0.5899\n",
      "Epoch [28/50], lter [80/90], Loss: 0.8236\n",
      "Epoch [29/50], lter [20/90], Loss: 0.8722\n",
      "Epoch [29/50], lter [40/90], Loss: 1.1812\n",
      "Epoch [29/50], lter [60/90], Loss: 0.9581\n",
      "Epoch [29/50], lter [80/90], Loss: 0.8704\n",
      "Epoch [30/50], lter [20/90], Loss: 1.0567\n",
      "Epoch [30/50], lter [40/90], Loss: 1.0472\n",
      "Epoch [30/50], lter [60/90], Loss: 1.0458\n",
      "Epoch [30/50], lter [80/90], Loss: 0.8851\n",
      "Epoch [31/50], lter [20/90], Loss: 0.8436\n",
      "Epoch [31/50], lter [40/90], Loss: 0.5981\n",
      "Epoch [31/50], lter [60/90], Loss: 0.9642\n",
      "Epoch [31/50], lter [80/90], Loss: 0.8537\n",
      "Epoch [32/50], lter [20/90], Loss: 1.0222\n",
      "Epoch [32/50], lter [40/90], Loss: 0.8444\n",
      "Epoch [32/50], lter [60/90], Loss: 0.9022\n",
      "Epoch [32/50], lter [80/90], Loss: 1.1011\n",
      "Epoch [33/50], lter [20/90], Loss: 0.6694\n",
      "Epoch [33/50], lter [40/90], Loss: 0.7854\n",
      "Epoch [33/50], lter [60/90], Loss: 0.8166\n",
      "Epoch [33/50], lter [80/90], Loss: 0.8234\n",
      "Epoch [34/50], lter [20/90], Loss: 0.9661\n",
      "Epoch [34/50], lter [40/90], Loss: 0.7931\n",
      "Epoch [34/50], lter [60/90], Loss: 0.9046\n",
      "Epoch [34/50], lter [80/90], Loss: 0.9730\n",
      "Epoch [35/50], lter [20/90], Loss: 0.8990\n",
      "Epoch [35/50], lter [40/90], Loss: 0.9978\n",
      "Epoch [35/50], lter [60/90], Loss: 0.9139\n",
      "Epoch [35/50], lter [80/90], Loss: 0.7319\n",
      "Epoch [36/50], lter [20/90], Loss: 0.7582\n",
      "Epoch [36/50], lter [40/90], Loss: 1.2194\n",
      "Epoch [36/50], lter [60/90], Loss: 0.6795\n",
      "Epoch [36/50], lter [80/90], Loss: 1.0503\n",
      "Epoch [37/50], lter [20/90], Loss: 0.8204\n",
      "Epoch [37/50], lter [40/90], Loss: 1.0612\n",
      "Epoch [37/50], lter [60/90], Loss: 1.0907\n",
      "Epoch [37/50], lter [80/90], Loss: 0.9762\n",
      "Epoch [38/50], lter [20/90], Loss: 1.0225\n",
      "Epoch [38/50], lter [40/90], Loss: 0.9369\n",
      "Epoch [38/50], lter [60/90], Loss: 0.8994\n",
      "Epoch [38/50], lter [80/90], Loss: 1.1634\n",
      "Epoch [39/50], lter [20/90], Loss: 1.2030\n",
      "Epoch [39/50], lter [40/90], Loss: 0.8206\n",
      "Epoch [39/50], lter [60/90], Loss: 0.6762\n",
      "Epoch [39/50], lter [80/90], Loss: 1.3544\n",
      "Epoch [40/50], lter [20/90], Loss: 0.6517\n",
      "Epoch [40/50], lter [40/90], Loss: 0.9763\n",
      "Epoch [40/50], lter [60/90], Loss: 0.7401\n",
      "Epoch [40/50], lter [80/90], Loss: 0.7024\n",
      "Epoch [41/50], lter [20/90], Loss: 0.9012\n",
      "Epoch [41/50], lter [40/90], Loss: 0.9002\n",
      "Epoch [41/50], lter [60/90], Loss: 0.9650\n",
      "Epoch [41/50], lter [80/90], Loss: 1.3795\n",
      "Epoch [42/50], lter [20/90], Loss: 0.8267\n",
      "Epoch [42/50], lter [40/90], Loss: 0.6878\n",
      "Epoch [42/50], lter [60/90], Loss: 0.8217\n",
      "Epoch [42/50], lter [80/90], Loss: 1.4655\n",
      "Epoch [43/50], lter [20/90], Loss: 0.9035\n",
      "Epoch [43/50], lter [40/90], Loss: 0.7962\n",
      "Epoch [43/50], lter [60/90], Loss: 0.8980\n",
      "Epoch [43/50], lter [80/90], Loss: 0.7823\n",
      "Epoch [44/50], lter [20/90], Loss: 0.8215\n",
      "Epoch [44/50], lter [40/90], Loss: 1.1662\n",
      "Epoch [44/50], lter [60/90], Loss: 0.8024\n",
      "Epoch [44/50], lter [80/90], Loss: 0.9931\n",
      "Epoch [45/50], lter [20/90], Loss: 0.8984\n",
      "Epoch [45/50], lter [40/90], Loss: 0.8988\n",
      "Epoch [45/50], lter [60/90], Loss: 0.7590\n",
      "Epoch [45/50], lter [80/90], Loss: 1.1433\n",
      "Epoch [46/50], lter [20/90], Loss: 0.9020\n",
      "Epoch [46/50], lter [40/90], Loss: 0.8461\n",
      "Epoch [46/50], lter [60/90], Loss: 0.6599\n",
      "Epoch [46/50], lter [80/90], Loss: 0.7960\n",
      "Epoch [47/50], lter [20/90], Loss: 0.7893\n",
      "Epoch [47/50], lter [40/90], Loss: 0.8080\n",
      "Epoch [47/50], lter [60/90], Loss: 0.7000\n",
      "Epoch [47/50], lter [80/90], Loss: 1.0647\n",
      "Epoch [48/50], lter [20/90], Loss: 0.7437\n",
      "Epoch [48/50], lter [40/90], Loss: 0.8573\n",
      "Epoch [48/50], lter [60/90], Loss: 0.7985\n",
      "Epoch [48/50], lter [80/90], Loss: 0.8996\n",
      "Epoch [49/50], lter [20/90], Loss: 0.9990\n",
      "Epoch [49/50], lter [40/90], Loss: 0.7087\n",
      "Epoch [49/50], lter [60/90], Loss: 1.1537\n",
      "Epoch [49/50], lter [80/90], Loss: 1.1473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], lter [20/90], Loss: 0.9012\n",
      "Epoch [50/50], lter [40/90], Loss: 1.2143\n",
      "Epoch [50/50], lter [60/90], Loss: 1.1420\n",
      "Epoch [50/50], lter [80/90], Loss: 0.9010\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, (batch_text, batch_labels) in enumerate(train_loader):\n",
    "        \n",
    "        X = batch_text.cuda()\n",
    "        Y = batch_labels.cuda()\n",
    "        \n",
    "        pre = model(X)\n",
    "        cost = loss(pre, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "    \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.6 Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "x_test = x_test.view(-1, 1, 12, 500)\n",
    "\n",
    "y_test = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "\n",
    "test_data = TensorDataset(x, y)\n",
    "\n",
    "test_loader  = DataLoader(dataset=test_data,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test text: 58.590308 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for text, labels in test_loader:\n",
    "    \n",
    "    text = text.cuda()\n",
    "    outputs = model(text)\n",
    "    \n",
    "    _, pre = torch.max(outputs.data, 1)\n",
    "    total += 1\n",
    "    correct += (pre == labels.cuda()).sum()\n",
    "    \n",
    "print('Accuracy of test text: %f %%' % (100 * float(correct) / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
